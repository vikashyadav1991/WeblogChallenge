{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import re\n",
    "# Load PySpark\n",
    "spark = SparkSession.builder.appName('Analysis').getOrCreate()\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing & Analytical goals:\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "regx = r\"^(\\S+) (\\S+) (\\S+) (\\S+) (\\S+) (\\S+) (\\S+) (\\S+) (\\S+) (\\S+) (\\S+) \\S+ (\\S+) \\S+ (.*) (\\S+) (\\S+)$\"\n",
    "rdd = sc.textFile(\"data/2015_07_22_mktplace_shop_web_log_sample.log.gz\").map(lambda x: re.split(regx, x)[1:16])\n",
    "rdd_ = rdd.map(lambda x: (x[0], x[1], x[2], x[3], float(x[4]), float(x[5]), float(x[6]), int(x[7]), int(x[8]), int(x[9]), int(x[10]), x[11], x[12], x[13], x[14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Schema = T.StructType([T.StructField(\"timestamp\", T.StringType(), True),\n",
    "                                    T.StructField(\"elb\", T.StringType(), True),\n",
    "                                    T.StructField(\"client_port\", T.StringType(), True),\n",
    "                                    T.StructField(\"backend_port\", T.StringType(), True),\n",
    "                                    T.StructField(\"request_processing_time\", T.DoubleType(), True),\n",
    "                                    T.StructField(\"backend_processing_time\", T.DoubleType(), True),\n",
    "                                    T.StructField(\"response_processing_time\", T.DoubleType(), True),\n",
    "                                    T.StructField(\"elb_status_code\", T.LongType(), True),\n",
    "                                    T.StructField(\"backend_status_code\", T.LongType(), True),\n",
    "                                    T.StructField(\"received_bytes\", T.LongType(), True),\n",
    "                                    T.StructField(\"sent_bytes\", T.LongType(), True),\n",
    "                                    T.StructField(\"request\", T.StringType(), True),\n",
    "                                    T.StructField(\"user_agent\", T.StringType(), True),\n",
    "                                    T.StructField(\"ssl_cipher\", T.StringType(), True),\n",
    "                                    T.StructField(\"ssl_protocol\", T.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv(\"data/2015_07_22_mktplace_shop_web_log_sample.log.gz\", Schema, sep = \" \", ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True).cache()\n",
    "df = spark.createDataFrame(rdd_, schema=Schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Sessionize the web log by IP. Sessionize = aggregrate all page hits by visitor/IP during a session.\n",
    "---------\n",
    "https://en.wikipedia.org/wiki/Session_(web_analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sessionize(df):\n",
    "    time_frame = 15 * 60  # 15 mins * 60\n",
    "    w = Window.partitionBy(\"client_ip\").orderBy(\"timestamp\")\n",
    "    diff = F.coalesce(F.unix_timestamp(F.col(\"timestamp\")) - F.unix_timestamp(F.lag(F.col(\"timestamp\"), 1).over(w)), F.lit(0))\n",
    "    cum_diff = F.sum(diff).over(w)\n",
    "    subgroup = (cum_diff / time_frame).cast('integer').alias(\"session_id\")\n",
    "    return df.select(\"*\", subgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the average session time\n",
    "def get_avg_session_time(df):\n",
    "    df_ = df.groupby(\"client_ip\", \"session_id\").agg(((F.unix_timestamp(F.max(\"timestamp\")) - F.unix_timestamp(F.min(\"timestamp\")) + 1)/60).alias(\"session_length\"))\n",
    "    avg_session_time = df_.agg(F.avg(\"session_length\").alias(\"avg_session_time\")).collect()[0][\"avg_session_time\"]\n",
    "    return avg_session_time\n",
    "\n",
    "# Find the most engaged users, ie the IPs with the longest session times\n",
    "def get_most_engaged_user(df):\n",
    "    df_ = df.groupby(\"client_ip\", \"session_id\").agg(F.min(\"timestamp\").alias(\"from_timestamp\"), F.max(\"timestamp\").alias(\"to_timestamp\"), ((F.unix_timestamp(F.max(\"timestamp\")) - F.unix_timestamp(F.min(\"timestamp\")) + 1)/60).alias(\"session_length\"))\n",
    "    return df_.orderBy(\"session_length\", ascending=False)\n",
    "\n",
    "# Determine unique URL visits per session. To clarify, count a hit to a unique URL only once per session.\n",
    "def get_unique_url_request(df):\n",
    "    return df.groupby(\"client_ip\", \"session_id\").agg(F.countDistinct(\"request\").alias(\"unique_url_request\"))\n",
    "\n",
    "def analyze(df):\n",
    "    df_ = df.groupby(\"client_ip\", \"session_id\").agg(F.min(\"timestamp\").alias(\"from_timestamp\"), F.max(\"timestamp\").alias(\"to_timestamp\"), ((F.unix_timestamp(F.max(\"timestamp\")) - F.unix_timestamp(F.min(\"timestamp\")) + 1)/60).alias(\"session_length\"), F.countDistinct(\"request\").alias(\"unique_url_request\"))\n",
    "    return df_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.withColumn(\"client_ip\", F.split(F.col(\"client_port\"), ':')[0])\\\n",
    ".withColumn(\"timestamp\", F.col(\"timestamp\").substr(0,19).cast('timestamp')).select(\"client_ip\", \"timestamp\", F.lower(F.col(\"request\")).alias(\"request\"))\n",
    "\n",
    "df_2 = sessionize(df_1)\n",
    "df_3 = analyze(df_2).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Determine the average session time\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Distinct Sessions = 115936\n",
      "Average Session Time (Minutes) = 1.3433538043978264\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Distinct Sessions = {}\".format(df_2.groupby(\"client_ip\", \"session_id\").count().count()))\n",
    "print(\"Average Session Time (Minutes) = {}\".format(df_3.agg(F.avg(\"session_length\").alias(\"avg_session_time\")).collect()[0][\"avg_session_time\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Determine unique URL visits per session. To clarify, count a hit to a unique URL only once per session.\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients which made request to only 1 unique URL = 25923\n",
      "\n",
      "20 Clients with most unique URL visits per session\n",
      "+-------------+-------------------+-------------------+------------------+\n",
      "|client_ip    |from_timestamp     |to_timestamp       |unique_url_request|\n",
      "+-------------+-------------------+-------------------+------------------+\n",
      "|119.81.61.166|2015-07-22 16:10:28|2015-07-22 16:25:05|8016              |\n",
      "|52.74.219.71 |2015-07-22 16:10:28|2015-07-22 16:25:05|5478              |\n",
      "|52.74.219.71 |2015-07-22 10:30:28|2015-07-22 10:39:47|5057              |\n",
      "|106.186.23.95|2015-07-22 21:05:28|2015-07-22 21:10:13|4320              |\n",
      "|119.81.61.166|2015-07-22 17:40:28|2015-07-22 17:45:28|3928              |\n",
      "|119.81.61.166|2015-07-22 18:00:28|2015-07-22 18:05:27|3637              |\n",
      "|119.81.61.166|2015-07-22 02:40:06|2015-07-22 02:45:03|3334              |\n",
      "|52.74.219.71 |2015-07-22 18:00:28|2015-07-22 18:05:27|2907              |\n",
      "|119.81.61.166|2015-07-22 21:05:28|2015-07-22 21:10:04|2807              |\n",
      "|119.81.61.166|2015-07-22 16:40:28|2015-07-22 16:44:48|2786              |\n",
      "|106.51.132.54|2015-07-22 16:10:28|2015-07-22 16:14:52|2609              |\n",
      "|52.74.219.71 |2015-07-22 09:00:28|2015-07-22 09:04:43|2572              |\n",
      "|52.74.219.71 |2015-07-22 10:45:28|2015-07-22 10:49:52|2569              |\n",
      "|52.74.219.71 |2015-07-22 17:40:28|2015-07-22 17:45:27|2535              |\n",
      "|52.74.219.71 |2015-07-22 16:40:28|2015-07-22 16:44:48|2458              |\n",
      "|52.74.219.71 |2015-07-22 21:05:28|2015-07-22 21:10:05|2318              |\n",
      "|52.74.219.71 |2015-07-22 05:10:07|2015-07-22 05:15:07|2037              |\n",
      "|52.74.219.71 |2015-07-22 11:00:28|2015-07-22 11:04:57|1958              |\n",
      "|119.81.61.166|2015-07-22 06:55:07|2015-07-22 07:00:07|1671              |\n",
      "|54.169.20.106|2015-07-22 16:11:48|2015-07-22 16:24:03|1611              |\n",
      "+-------------+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of clients which made request to only 1 unique URL = {}\".format(df_3.where(\"unique_url_request = 1\").count()))\n",
    "\n",
    "print()\n",
    "print(\"20 Clients with most unique URL visits per session\")\n",
    "df_3.select(\"client_ip\", \"from_timestamp\", \"to_timestamp\", \"unique_url_request\").sort(\"unique_url_request\", ascending=False).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Find the most engaged users, ie the IPs with the longest session times.\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing 20 Most Engaged Users with there session start and end time, session length in minutes\n",
      "+---------------+-------------------+-------------------+--------------+\n",
      "|client_ip      |from_timestamp     |to_timestamp       |session_length|\n",
      "+---------------+-------------------+-------------------+--------------+\n",
      "|120.56.178.102 |2015-07-22 10:45:54|2015-07-22 11:00:53|15.0          |\n",
      "|112.79.37.154  |2015-07-22 10:45:42|2015-07-22 11:00:41|15.0          |\n",
      "|103.29.159.62  |2015-07-22 10:45:35|2015-07-22 11:00:34|15.0          |\n",
      "|122.176.150.156|2015-07-22 10:31:43|2015-07-22 10:46:42|15.0          |\n",
      "|106.76.143.18  |2015-07-22 10:46:34|2015-07-22 11:01:33|15.0          |\n",
      "|220.227.161.206|2015-07-22 10:34:11|2015-07-22 10:49:10|15.0          |\n",
      "|117.218.65.155 |2015-07-22 10:46:47|2015-07-22 11:01:46|15.0          |\n",
      "|180.188.249.253|2015-07-22 10:46:23|2015-07-22 11:01:22|15.0          |\n",
      "|14.99.239.11   |2015-07-22 10:34:37|2015-07-22 10:49:36|15.0          |\n",
      "|165.241.31.254 |2015-07-22 10:48:58|2015-07-22 11:03:57|15.0          |\n",
      "|117.218.41.153 |2015-07-22 10:31:02|2015-07-22 10:46:01|15.0          |\n",
      "|125.16.12.194  |2015-07-22 10:47:57|2015-07-22 11:02:56|15.0          |\n",
      "|115.249.50.242 |2015-07-22 10:30:56|2015-07-22 10:45:55|15.0          |\n",
      "|103.251.16.184 |2015-07-22 10:31:37|2015-07-22 10:46:36|15.0          |\n",
      "|164.100.123.241|2015-07-22 10:30:49|2015-07-22 10:45:48|15.0          |\n",
      "|122.166.229.150|2015-07-22 10:33:53|2015-07-22 10:48:52|15.0          |\n",
      "|117.199.107.47 |2015-07-22 10:48:09|2015-07-22 11:03:08|15.0          |\n",
      "|117.223.81.194 |2015-07-22 10:30:30|2015-07-22 10:45:29|15.0          |\n",
      "|182.156.100.159|2015-07-22 10:46:26|2015-07-22 11:01:25|15.0          |\n",
      "|115.245.254.253|2015-07-22 10:32:20|2015-07-22 10:47:19|15.0          |\n",
      "+---------------+-------------------+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing 20 Most Engaged Users with there session start and end time, session length in minutes\")\n",
    "df_3.select(\"client_ip\", \"from_timestamp\", \"to_timestamp\", \"session_length\").sort(\"session_length\", ascending=False).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional questions for Machine Learning Engineer (MLE) candidates:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t_1 = df.withColumn(\"client_ip\", F.split(F.col(\"client_port\"), ':')[0])\\\n",
    ".withColumn(\"hour\", df.timestamp.substr(12,2).cast('integer'))\\\n",
    ".withColumn(\"minute\", df.timestamp.substr(15,2).cast('integer'))\\\n",
    ".withColumn(\"timestamp\", F.col(\"timestamp\").substr(0,16).cast('timestamp'))\n",
    "\n",
    "df_t_2 = df_t_1.groupby(\"timestamp\").agg((F.count(\"timestamp\")/60).alias(\"load\"), \n",
    "                                F.avg(\"received_bytes\").alias(\"received_bytes\"), \n",
    "                                F.avg(\"sent_bytes\").alias(\"sent_bytes\"), \n",
    "                                F.countDistinct(\"request\").alias(\"unique_request_count\"), \n",
    "                                F.countDistinct(\"client_ip\").alias(\"unique_ip_count\"), \n",
    "                                F.first(\"hour\").alias(\"hour\"), \n",
    "                                F.first(\"minute\").alias(\"minute\")).sort([\"hour\", \"minute\"]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy().orderBy([\"hour\", \"minute\"])\n",
    "df_t_3 = df_t_2.select(\"hour\", \"minute\", F.lag(\"sent_bytes\").over(w).alias(\"prev_sent_bytes\"), F.lag(\"received_bytes\").over(w).alias(\"prev_received_bytes\"), F.lag(\"unique_request_count\").over(w).alias(\"prev_unique_request_count\"), F.lag(\"unique_ip_count\").over(w).alias(\"prev_unique_ip_count\"), \"load\").where(F.col(\"prev_sent_bytes\").isNotNull()).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"hour\", \"minute\", \"prev_sent_bytes\", \"prev_received_bytes\", \n",
    "                                       \"prev_unique_request_count\", \"prev_unique_ip_count\"], outputCol=\"vectorized\")\n",
    "\n",
    "df_t_4 = assembler.transform(df_t_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_t_4.randomSplit([0.8, 0.2], seed=2019)\n",
    "training = train.withColumn(\"label\", F.col(\"load\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Predict the expected load (requests/second) in the next minute\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "polyExpansion = PolynomialExpansion(inputCol=\"vectorized\", outputCol=\"features\")\n",
    "lr = LinearRegression(maxIter=100, featuresCol=\"features\", labelCol=\"label\")\n",
    "pipeline = Pipeline(stages=[polyExpansion, lr])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(polyExpansion.degree, [1, 2, 3])\\\n",
    ".addGrid(lr.regParam, [0.1, 0.01, 0.001])\\\n",
    ".addGrid(lr.fitIntercept, [False, True])\\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    ".build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=RegressionEvaluator(), numFolds=5)\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model degree of polynomail = 1\n",
      "Best model regParam value = 0.1\n",
      "Best model fitIntercept value = True\n",
      "Best model elasticNetParam value = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model degree of polynomail = {}\".format(cvModel.bestModel.stages[0]._java_obj.getDegree()))\n",
    "print(\"Best model regParam value = {}\".format(cvModel.bestModel.stages[1]._java_obj.getRegParam()))\n",
    "print(\"Best model fitIntercept value = {}\".format(cvModel.bestModel.stages[1]._java_obj.getFitIntercept()))\n",
    "print(\"Best model elasticNetParam value = {}\".format(cvModel.bestModel.stages[1]._java_obj.getElasticNetParam()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|load                |prediction         |\n",
      "+--------------------+-------------------+\n",
      "|78.0                |79.86522967288911  |\n",
      "|25.55               |192.06314501523212 |\n",
      "|0.06666666666666667 |36.33495495168938  |\n",
      "|224.55              |179.47996270635434 |\n",
      "|390.18333333333334  |360.7445611799494  |\n",
      "|207.35              |120.51825269450566 |\n",
      "|410.7               |305.6405420405466  |\n",
      "|348.5833333333333   |326.55421616874145 |\n",
      "|192.83333333333334  |80.81338313203538  |\n",
      "|405.35              |208.48229686086225 |\n",
      "|381.4166666666667   |322.43855347671536 |\n",
      "|0.5                 |-19.939321071743336|\n",
      "|0.5166666666666667  |8.100657726289029  |\n",
      "|0.85                |70.77168548518706  |\n",
      "|0.03333333333333333 |68.31719307735177  |\n",
      "|350.8666666666667   |315.53061575573884 |\n",
      "|33.0                |330.00454435468475 |\n",
      "|0.016666666666666666|70.36889737819303  |\n",
      "|281.9               |336.5469843692852  |\n",
      "|0.03333333333333333 |267.751740345182   |\n",
      "|351.71666666666664  |159.50529471115576 |\n",
      "|241.26666666666668  |173.25768158691847 |\n",
      "|304.1166666666667   |290.92569102227736 |\n",
      "|272.3333333333333   |293.65530730256046 |\n",
      "|139.11666666666667  |285.86093063592693 |\n",
      "|0.06666666666666667 |0.7846302285927322 |\n",
      "|55.0                |124.8272159895252  |\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(\"load\", \"prediction\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 99.109393\n",
      "r2: 0.582128\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = cvModel.bestModel.stages[1].summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "t = spark.createDataFrame([Row(date='2016-01-01', get_avg=5, get_first=1),\n",
    "                            Row(date='2016-01-01', get_avg=5, get_first=2),\n",
    "                            Row(date='2016-01-02', get_avg=10, get_first=3),\n",
    "                            Row(date='2016-01-02', get_avg=20, get_first=3),\n",
    "                            Row(date='2016-01-10', get_avg=30, get_first=3),\n",
    "                            Row(date='2016-01-10', get_avg=10, get_first=3),\n",
    "                            Row(date='2016-01-10', get_avg=20, get_first=3),\n",
    "                            Row(date='2016-01-12', get_avg=30, get_first=3),\n",
    "                            Row(date='2016-01-12', get_avg=8, get_first=4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|date      |get_avg|\n",
      "+----------+-------+\n",
      "|2016-01-01|5.0    |\n",
      "|2016-01-02|15.0   |\n",
      "|2016-01-10|20.0   |\n",
      "|2016-01-12|19.0   |\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_ = t.groupby(\"date\").agg(F.avg(\"get_avg\").alias(\"get_avg\")).sort(\"date\")\n",
    "t_.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy().orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------------------------------------------------------------------------+\n",
      "|      date|get_avg|lag(get_avg, 2, NULL) OVER (ORDER BY date ASC NULLS FIRST unspecifiedframe$())|\n",
      "+----------+-------+------------------------------------------------------------------------------+\n",
      "|2016-01-01|    5.0|                                                                          null|\n",
      "|2016-01-02|   15.0|                                                                          null|\n",
      "|2016-01-10|   20.0|                                                                           5.0|\n",
      "|2016-01-12|   19.0|                                                                          15.0|\n",
      "+----------+-------+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_.select(\"*\", F.lag(t_.get_avg, 2).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------------+\n",
      "|date      |datediff(date, CAST(2016-01-09 AS DATE))|\n",
      "+----------+----------------------------------------+\n",
      "|2016-01-01|-8                                      |\n",
      "|2016-01-01|-8                                      |\n",
      "|2016-01-02|-7                                      |\n",
      "|2016-01-02|-7                                      |\n",
      "|2016-01-10|1                                       |\n",
      "|2016-01-10|1                                       |\n",
      "|2016-01-10|1                                       |\n",
      "|2016-01-12|3                                       |\n",
      "|2016-01-12|3                                       |\n",
      "+----------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.date, F.datediff(F.col(\"date\"), F.lit('2016-01-09').cast('date'))).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "analyzedDataset": "2015_07_22_mktplace_shop_web_log_sample_sorted",
  "creator": "admin",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
